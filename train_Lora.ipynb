{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2354e3-3328-4d62-9653-ccb15e17850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346494d-a209-4d50-895b-81a94db89403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# peft_config = LoraConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM, \n",
    "#     inference_mode=False, \n",
    "#     r=8, \n",
    "#     lora_alpha=32, \n",
    "#     lora_dropout=0.1,\n",
    "#     target_modules=['query_key_value']\n",
    "# )\n",
    "# # 获取基础模型\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     \"THUDM/chatglm-6b\", trust_remote_code=True, device_map=\"auto\"\n",
    "# )\n",
    "# # 加入peft config\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# # 打印参数情况\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16951ea-1af9-4c63-9d1c-05ed658bdc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c448ec2-c1cb-4609-ad41-6d9b5d8884bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c5db85-8063-475a-8d92-f7b50094fba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-11-29 20:54:21,894] torch.distributed.elastic.multiprocessing.redirects: [WARNING] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "from tensorfn import load_config as DiffConfig\n",
    "import numpy as np\n",
    "from config.diffconfig import DiffusionConfig, get_model_conf\n",
    "import torch.distributed as dist\n",
    "import os, glob, cv2, time, shutil\n",
    "from models.unet_autoenc import BeatGANsAutoencConfig\n",
    "from diffusion import create_gaussian_diffusion, make_beta_schedule, ddim_steps\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c7fe92-44a8-4d09-9bce-3df42117508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = DiffConfig(DiffusionConfig, './config/diffusion.conf', show=False)\n",
    "\n",
    "model = get_model_conf().make_model()\n",
    "ckpt = torch.load(\"checkpoints/last.pt\")\n",
    "model.load_state_dict(ckpt[\"ema\"])\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "betas = conf.diffusion.beta_schedule.make()\n",
    "diffusion = create_gaussian_diffusion(betas, predict_xstart = False)#.to(device)\n",
    "\n",
    "pose_list = glob.glob('data/deepfashion_256x256/target_pose/*.npy')\n",
    "transforms = transforms.Compose([transforms.Resize((256,256), interpolation=Image.BICUBIC),\n",
    "                    transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                        (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出模型的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_embed <class 'models.unet_autoenc.TimeStyleSeperateEmbed'>\n",
      "input_blocks <class 'torch.nn.modules.container.ModuleList'>\n",
      "middle_block <class 'models.blocks.TimestepEmbedSequential'>\n",
      "output_blocks <class 'torch.nn.modules.container.ModuleList'>\n",
      "out <class 'torch.nn.modules.container.Sequential'>\n",
      "encoder <class 'models.unet.BeatGANsEncoder'>\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_children():\n",
    "    print(name, type(layer), sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_embed TimeStyleSeperateEmbed(\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (style): Identity()\n",
      ")\n",
      "input_blocks ModuleList(\n",
      "  (0): TimestepEmbedSequential(\n",
      "    (0): Conv2d(23, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (1-2): 2 x TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (3): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (x_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (4-5): 2 x TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (6): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (x_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (7): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (8): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (9): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (x_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (10): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (11): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "    (1): AttentionBlock(\n",
      "      (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "      (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
      "      (to_kv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "      (to_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (selfattention): QKVAttentionLegacy()\n",
      "      (crossattention): QKVAttentionLegacy()\n",
      "      (proj_out1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (proj_out2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (12): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (x_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (13): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (14): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "    (1): AttentionBlock(\n",
      "      (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "      (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (to_kv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (to_q): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      (selfattention): QKVAttentionLegacy()\n",
      "      (crossattention): QKVAttentionLegacy()\n",
      "      (proj_out1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      (proj_out2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (15): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (x_upd): Downsample(\n",
      "        (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "      )\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (16): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (17): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "    (1): AttentionBlock(\n",
      "      (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "      (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (to_kv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (to_q): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      (selfattention): QKVAttentionLegacy()\n",
      "      (crossattention): QKVAttentionLegacy()\n",
      "      (proj_out1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      (proj_out2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "middle_block_0 ResBlock(\n",
      "  (in_layers): Sequential(\n",
      "    (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "    (1): SiLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (h_upd): Identity()\n",
      "  (x_upd): Identity()\n",
      "  (emb_layers): Sequential(\n",
      "    (0): SiLU()\n",
      "    (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cond_emb_layers): Sequential(\n",
      "    (0): SiLU()\n",
      "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (out_layers): Sequential(\n",
      "    (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "    (1): SiLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (skip_connection): Identity()\n",
      ")\n",
      "middle_block_1 AttentionBlock(\n",
      "  (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "  (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
      "  (to_kv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "  (to_q): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "  (selfattention): QKVAttentionLegacy()\n",
      "  (crossattention): QKVAttentionLegacy()\n",
      "  (proj_out1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "  (proj_out2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      ")\n",
      "middle_block_2 ResBlock(\n",
      "  (in_layers): Sequential(\n",
      "    (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "    (1): SiLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (h_upd): Identity()\n",
      "  (x_upd): Identity()\n",
      "  (emb_layers): Sequential(\n",
      "    (0): SiLU()\n",
      "    (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  )\n",
      "  (cond_emb_layers): Sequential(\n",
      "    (0): SiLU()\n",
      "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (out_layers): Sequential(\n",
      "    (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "    (1): SiLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (skip_connection): Identity()\n",
      ")\n",
      "output_blocks ModuleList(\n",
      "  (0): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (1): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): AttentionBlock(\n",
      "      (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "      (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (to_kv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (to_q): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      (selfattention): QKVAttentionLegacy()\n",
      "      (crossattention): QKVAttentionLegacy()\n",
      "      (proj_out1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      (proj_out2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (2): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Upsample()\n",
      "      (x_upd): Upsample()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (3): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (4): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 1024, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): AttentionBlock(\n",
      "      (norm): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "      (qkv): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))\n",
      "      (to_kv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (to_q): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      (selfattention): QKVAttentionLegacy()\n",
      "      (crossattention): QKVAttentionLegacy()\n",
      "      (proj_out1): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "      (proj_out2): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (5): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Upsample()\n",
      "      (x_upd): Upsample()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (6): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 768, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (7): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): AttentionBlock(\n",
      "      (norm): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "      (qkv): Conv1d(256, 768, kernel_size=(1,), stride=(1,))\n",
      "      (to_kv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "      (to_q): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (selfattention): QKVAttentionLegacy()\n",
      "      (crossattention): QKVAttentionLegacy()\n",
      "      (proj_out1): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "      (proj_out2): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (8): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Upsample()\n",
      "      (x_upd): Upsample()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (9-10): 2 x TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (11): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Upsample()\n",
      "      (x_upd): Upsample()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (12): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 384, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (13): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (14): TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Upsample()\n",
      "      (x_upd): Upsample()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Identity()\n",
      "    )\n",
      "  )\n",
      "  (15-17): 3 x TimestepEmbedSequential(\n",
      "    (0): ResBlock(\n",
      "      (in_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (h_upd): Identity()\n",
      "      (x_upd): Identity()\n",
      "      (emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "      )\n",
      "      (cond_emb_layers): Sequential(\n",
      "        (0): SiLU()\n",
      "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (out_layers): Sequential(\n",
      "        (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "        (1): SiLU()\n",
      "        (2): Dropout(p=0.1, inplace=False)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (skip_connection): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "out_0 GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "out_1 SiLU()\n",
      "out_2 Conv2d(128, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "encoder BeatGANsEncoder(\n",
      "  (time_embed): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (1): SiLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (input_blocks): ModuleList(\n",
      "    (0): TimestepEmbedSequential(\n",
      "      (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (1-2): 2 x TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Identity()\n",
      "        (x_upd): Identity()\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (3): TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (x_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (4-5): 2 x TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Identity()\n",
      "        (x_upd): Identity()\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (6): TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (x_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (7): TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 128, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Identity()\n",
      "        (x_upd): Identity()\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (8): TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Identity()\n",
      "        (x_upd): Identity()\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (9): TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (x_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (10-11): 2 x TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Identity()\n",
      "        (x_upd): Identity()\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (12): TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (x_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (13): TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 256, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Identity()\n",
      "        (x_upd): Identity()\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (14): TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Identity()\n",
      "        (x_upd): Identity()\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (15): TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (x_upd): Downsample(\n",
      "          (op): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "        )\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "    (16-17): 2 x TimestepEmbedSequential(\n",
      "      (0): ResBlock(\n",
      "        (in_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (h_upd): Identity()\n",
      "        (x_upd): Identity()\n",
      "        (emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        )\n",
      "        (cond_emb_layers): Sequential(\n",
      "          (0): SiLU()\n",
      "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (out_layers): Sequential(\n",
      "          (0): GroupNorm32(32, 512, eps=1e-05, affine=True)\n",
      "          (1): SiLU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (skip_connection): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def flatten(module):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, torch.nn.Sequential):\n",
    "            for sub_name, sub_child in flatten(child):\n",
    "                yield (f'{name}_{sub_name}', sub_child)\n",
    "        else:\n",
    "            yield (name, child)\n",
    "\n",
    "for (name, layer) in flatten(model):\n",
    "    print(name, layer, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_feature_size',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'conf',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dump_patches',\n",
       " 'encode',\n",
       " 'encode_stylespace',\n",
       " 'encoder',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'forward_with_cond_scale',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'input_blocks',\n",
       " 'input_num_blocks',\n",
       " 'ipu',\n",
       " 'load_state_dict',\n",
       " 'middle_block',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'noise_to_cond',\n",
       " 'num_heads_upsample',\n",
       " 'out',\n",
       " 'output_blocks',\n",
       " 'output_num_blocks',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'reparameterize',\n",
       " 'requires_grad_',\n",
       " 'sample_z',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'stylespace_sizes',\n",
       " 'time_emb_channels',\n",
       " 'time_embed',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80cc7876-e1f2-4a6e-b1a9-f60e54f3137a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0312,  0.0318, -0.0328,  ...,  0.0256,  0.0114,  0.0340],\n",
      "        [-0.0187, -0.0168, -0.0417,  ...,  0.0033,  0.0139,  0.0299],\n",
      "        [ 0.0384,  0.0202,  0.0209,  ...,  0.0371,  0.0057, -0.0054],\n",
      "        ...,\n",
      "        [ 0.0182, -0.0258, -0.0025,  ..., -0.0131,  0.0229,  0.0422],\n",
      "        [-0.0376,  0.0151, -0.0044,  ..., -0.0338, -0.0212,  0.0122],\n",
      "        [ 0.0099,  0.0319, -0.0202,  ..., -0.0100,  0.0373,  0.0049]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name,param in  model.named_parameters():\n",
    "    #print(name)\n",
    "    if(name=='output_blocks.1.0.cond_emb_layers.1.weight'):\n",
    "        print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "636afeff-b84c-4566-a383-1f990a1af805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = LoraConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM, \n",
    "#     inference_mode=False, \n",
    "#     r=8, \n",
    "#     lora_alpha=32, \n",
    "#     lora_dropout=0.1,\n",
    "#     target_modules=['output_blocks.1.1.qkv']\n",
    "# )\n",
    "peft_config=LoraConfig(target_modules=['output_blocks.1.0.cond_emb_layers.1'])\n",
    "\n",
    "model=get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import train_LoRA as tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added by yehui\n",
    "os.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\"\n",
    "os.environ[\"WANDB_API_KEY\"] = '093f8148cd98c7bf349917fdccb11942e3b292e2' \n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"local_rank\"] = \"0\"\n",
    "os.environ['TORCH_DISTRIBUTED_ELASTIC_LOG_REDIRECT'] = 'FALSE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: pidm_LoRA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='help')\n",
    "parser.add_argument('--exp_name', type=str, default='pidm_deepfashion')\n",
    "parser.add_argument('--DiffConfigPath', type=str, default='./config/diffusion.conf')\n",
    "parser.add_argument('--DataConfigPath', type=str, default='./config/data.yaml')\n",
    "parser.add_argument('--dataset_path', type=str, default='./dataset/deepfashion2')\n",
    "parser.add_argument('--save_path', type=str, default='checkpoints')\n",
    "parser.add_argument('--cond_scale', type=int, default=2)\n",
    "parser.add_argument('--guidance_prob', type=int, default=0.1)\n",
    "parser.add_argument('--sample_algorithm', type=str, default='ddim') # ddpm, ddim\n",
    "parser.add_argument('--batch_size', type=int, default=2)#训练的时候会除以2，不知道为什么\n",
    "parser.add_argument('--save_wandb_logs_every_iters', type=int, default=50)\n",
    "parser.add_argument('--save_checkpoints_every_iters', type=int, default=2000)\n",
    "parser.add_argument('--save_wandb_images_every_epochs', type=int, default=10)\n",
    "parser.add_argument('--device', type=str, default='cuda')\n",
    "parser.add_argument('--n_gpu', type=int, default=1)\n",
    "parser.add_argument('--n_machine', type=int, default=1)\n",
    "parser.add_argument('--local_rank', type=int, default=0)\n",
    "parser.add_argument(\"opts\", default=None, nargs=argparse.REMAINDER)\n",
    "\n",
    "args = parser.parse_args(args=['--exp_name','pidm_LoRA'])\n",
    "\n",
    "print ('Experiment: '+ args.exp_name)\n",
    "DiffConf = tr.DiffConfig(DiffusionConfig,  args.DiffConfigPath, args.opts, False)\n",
    "DataConf = tr.DataConfig(args.DataConfigPath)\n",
    "\n",
    "\n",
    "DiffConf.training.ckpt_path = os.path.join(args.save_path, args.exp_name)\n",
    "DataConf.data.path = args.dataset_path\n",
    "\n",
    "\n",
    "if tr.is_main_process():\n",
    "\n",
    "    if not os.path.isdir(args.save_path): os.mkdir(args.save_path)\n",
    "    if not os.path.isdir(DiffConf.training.ckpt_path): os.mkdir(DiffConf.training.ckpt_path)\n",
    "\n",
    "#DiffConf.ckpt = \"checkpoints/last.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val dataset [data.fashion_data::Dataset] of size 8570 was created\n",
      "training dataset [data.fashion_data::Dataset] of size 37016 was created\n",
      "#Epoch - 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/37016 [00:32<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.23 GiB is allocated by PyTorch, and 12.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\train_Lora.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Documents/PythonScripts/PIDM/train_Lora.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tr\u001b[39m.\u001b[39;49mmain(model\u001b[39m=\u001b[39;49mmodel,settings \u001b[39m=\u001b[39;49m [args, DiffConf, DataConf], EXP_NAME \u001b[39m=\u001b[39;49m args\u001b[39m.\u001b[39;49mexp_name)\n",
      "File \u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\train_LoRA.py:305\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(model, settings, EXP_NAME)\u001b[0m\n\u001b[0;32m    302\u001b[0m betas \u001b[39m=\u001b[39m DiffConf\u001b[39m.\u001b[39mdiffusion\u001b[39m.\u001b[39mbeta_schedule\u001b[39m.\u001b[39mmake()\n\u001b[0;32m    303\u001b[0m diffusion \u001b[39m=\u001b[39m create_gaussian_diffusion(betas, predict_xstart \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 305\u001b[0m train(\n\u001b[0;32m    306\u001b[0m     DiffConf, train_dataset, val_dataset, model, ema, diffusion, betas, optimizer, scheduler, args\u001b[39m.\u001b[39;49mguidance_prob, args\u001b[39m.\u001b[39;49mcond_scale, args\u001b[39m.\u001b[39;49mdevice, wandb\n\u001b[0;32m    307\u001b[0m )\n",
      "File \u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\train_LoRA.py:139\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(conf, loader, val_loader, model, ema, diffusion, betas, optimizer, scheduler, guidance_prob, cond_scale, device, wandb)\u001b[0m\n\u001b[0;32m    131\u001b[0m target_pose \u001b[39m=\u001b[39m target_pose\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    132\u001b[0m time_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\n\u001b[0;32m    133\u001b[0m     \u001b[39m0\u001b[39m,\n\u001b[0;32m    134\u001b[0m     conf\u001b[39m.\u001b[39mdiffusion\u001b[39m.\u001b[39mbeta_schedule[\u001b[39m\"\u001b[39m\u001b[39mn_timestep\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    135\u001b[0m     (img\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],),\n\u001b[0;32m    136\u001b[0m     device\u001b[39m=\u001b[39mdevice,\n\u001b[0;32m    137\u001b[0m )\n\u001b[1;32m--> 139\u001b[0m loss_dict \u001b[39m=\u001b[39m diffusion\u001b[39m.\u001b[39;49mtraining_losses(model, x_start \u001b[39m=\u001b[39;49m target_img, t \u001b[39m=\u001b[39;49m time_t, cond_input \u001b[39m=\u001b[39;49m [img, target_pose], prob \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m guidance_prob)\n\u001b[0;32m    141\u001b[0m loss \u001b[39m=\u001b[39m loss_dict[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n\u001b[0;32m    142\u001b[0m loss_mse \u001b[39m=\u001b[39m loss_dict[\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\diffusion.py:982\u001b[0m, in \u001b[0;36mGaussianDiffusion.training_losses\u001b[1;34m(self, model, x_start, cond_input, t, prob, model_kwargs, noise)\u001b[0m\n\u001b[0;32m    980\u001b[0m         terms[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps\n\u001b[0;32m    981\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_type \u001b[39m==\u001b[39m LossType\u001b[39m.\u001b[39mMSE \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_type \u001b[39m==\u001b[39m LossType\u001b[39m.\u001b[39mRESCALED_MSE:\n\u001b[1;32m--> 982\u001b[0m     model_output \u001b[39m=\u001b[39m model(x \u001b[39m=\u001b[39;49m torch\u001b[39m.\u001b[39;49mcat([x_t, target_pose],\u001b[39m1\u001b[39;49m), t \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scale_timesteps(t), x_cond \u001b[39m=\u001b[39;49m img, prob \u001b[39m=\u001b[39;49m prob)\n\u001b[0;32m    985\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_var_type \u001b[39min\u001b[39;00m [\n\u001b[0;32m    986\u001b[0m         ModelVarType\u001b[39m.\u001b[39mLEARNED,\n\u001b[0;32m    987\u001b[0m         ModelVarType\u001b[39m.\u001b[39mLEARNED_RANGE,\n\u001b[0;32m    988\u001b[0m     ]:\n\u001b[0;32m    989\u001b[0m         B, C \u001b[39m=\u001b[39m x_t\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32me:\\Develop\\anaconda3\\envs\\PIDM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Develop\\anaconda3\\envs\\PIDM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\models\\unet_autoenc.py:264\u001b[0m, in \u001b[0;36mBeatGANsAutoencModel.forward\u001b[1;34m(self, x, t, x_cond, prob, y, cond, style, noise, t_cond, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m             lateral \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    262\u001b[0m             \u001b[39m# print(i, j, lateral)\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m         h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_blocks[k](h,\n\u001b[0;32m    265\u001b[0m                                   emb\u001b[39m=\u001b[39;49mdec_time_emb,\n\u001b[0;32m    266\u001b[0m                                   cond\u001b[39m=\u001b[39;49mdec_cond_emb[\u001b[39m-\u001b[39;49mk\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[0;32m    267\u001b[0m                                   lateral\u001b[39m=\u001b[39;49mlateral)\n\u001b[0;32m    270\u001b[0m         k \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    272\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(h)\n",
      "File \u001b[1;32me:\\Develop\\anaconda3\\envs\\PIDM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Develop\\anaconda3\\envs\\PIDM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\models\\blocks.py:39\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[1;34m(self, x, emb, cond, lateral)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, TimestepBlock):\n\u001b[1;32m---> 39\u001b[0m         x \u001b[39m=\u001b[39m layer(x, emb\u001b[39m=\u001b[39;49memb, cond\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, lateral\u001b[39m=\u001b[39;49mlateral)\n\u001b[0;32m     40\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, AttentionBlock):\n\u001b[0;32m     41\u001b[0m         x \u001b[39m=\u001b[39m layer(x, cond)\n",
      "File \u001b[1;32me:\\Develop\\anaconda3\\envs\\PIDM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32me:\\Develop\\anaconda3\\envs\\PIDM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\models\\blocks.py:197\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[1;34m(self, x, emb, cond, lateral)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, emb\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, cond\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lateral\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    190\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39m    Apply the block to a Tensor, conditioned on a timestep embedding.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39m        lateral: lateral connection from the encoder\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     \u001b[39mreturn\u001b[39;00m torch_checkpoint(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward, (x, emb, cond, lateral),\n\u001b[0;32m    198\u001b[0m                             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconf\u001b[39m.\u001b[39;49muse_checkpoint)\n",
      "File \u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\models\\nn.py:136\u001b[0m, in \u001b[0;36mtorch_checkpoint\u001b[1;34m(func, args, flag, preserve_rng_state)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    134\u001b[0m         func, \u001b[39m*\u001b[39margs, preserve_rng_state\u001b[39m=\u001b[39mpreserve_rng_state)\n\u001b[0;32m    135\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\models\\blocks.py:251\u001b[0m, in \u001b[0;36mResBlock._forward\u001b[1;34m(self, x, emb, cond, lateral)\u001b[0m\n\u001b[0;32m    248\u001b[0m         cond_out \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    250\u001b[0m     \u001b[39m# this is the new refactored code\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m     h \u001b[39m=\u001b[39m apply_conditions(\n\u001b[0;32m    252\u001b[0m         h\u001b[39m=\u001b[39;49mh,\n\u001b[0;32m    253\u001b[0m         emb\u001b[39m=\u001b[39;49memb_out,\n\u001b[0;32m    254\u001b[0m         cond\u001b[39m=\u001b[39;49mcond_out,\n\u001b[0;32m    255\u001b[0m         layers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_layers,\n\u001b[0;32m    256\u001b[0m         scale_bias\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m    257\u001b[0m         in_channels\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconf\u001b[39m.\u001b[39;49mout_channels,\n\u001b[0;32m    258\u001b[0m         up_down_layer\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_connection(x) \u001b[39m+\u001b[39m h\n",
      "File \u001b[1;32me:\\Documents\\PythonScripts\\PIDM\\models\\blocks.py:330\u001b[0m, in \u001b[0;36mapply_conditions\u001b[1;34m(h, emb, cond, layers, scale_bias, in_channels, up_down_layer)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[39mfor\u001b[39;00m i, (scale, shift) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(scale_shifts):\n\u001b[0;32m    328\u001b[0m     \u001b[39m# if scale is None, it indicates that the condition is not provided\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[39mif\u001b[39;00m scale \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         h \u001b[39m=\u001b[39m h \u001b[39m*\u001b[39;49m (biases[i] \u001b[39m+\u001b[39;49m scale)\n\u001b[0;32m    331\u001b[0m         \u001b[39mif\u001b[39;00m shift \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m             h \u001b[39m=\u001b[39m h \u001b[39m+\u001b[39m shift\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.23 GiB is allocated by PyTorch, and 12.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "tr.main(model=model,settings = [args, DiffConf, DataConf], EXP_NAME = args.exp_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
